{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets \n",
    "from sklearn.datasets import load_digits \n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# data processing\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# implemtation\n",
    "import numpy as np\n",
    "import numpy.random as r \n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "\n",
    "def load_mnist_data(source=\"sk\"):\n",
    "    if source == \"sk\":\n",
    "        digits=load_digits()\n",
    "        X = digits.data\n",
    "        y = digits.target\n",
    "    else:\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        X = np.append(x_train, x_test, axis=0)\n",
    "        X = X.reshape(-1, X.shape[1]**2)\n",
    "        y = np.append(y_train, y_test, axis=0)\n",
    "        \n",
    "    X_scale = StandardScaler()\n",
    "    X = X_scale.fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    y_train = convert_y_to_vect(y_train)\n",
    "#     y_test = convert_y_to_vect(y_test)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Implementation\n",
    "\n",
    "We have minified the implementation done in our homework and adapted it suit our needs for running the desired set of hyperparameter combinations and easily add extension modules to our naive implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializers(nn_structure, acti=\"relu\", mode=\"weights\"):\n",
    "    W = {}\n",
    "    b = {}\n",
    "    dW = {}\n",
    "    db = {} \n",
    "    VdW = {}\n",
    "    Vdb = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        if mode==\"weights\":\n",
    "            [low, high, inter] = [0., 1., 0.]\n",
    "            if acti==\"sigmoid\":\n",
    "                inter = np.sqrt((6)/(nn_structure[l] + nn_structure[l-1]))\n",
    "            elif acti==\"relu\":\n",
    "                inter = np.sqrt((6.)/float(nn_structure[l] + nn_structure[l-1]))\n",
    "\n",
    "            W[l] = r.uniform(low=-inter, high=inter, size=(nn_structure[l], nn_structure[l-1])) \n",
    "            b[l] = r.uniform(low=-inter, high=inter, size=(nn_structure[l],))\n",
    "        elif mode==\"gradients\":\n",
    "            dW[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "            db[l] = np.zeros((nn_structure[l],))\n",
    "        elif mode==\"momentum\":\n",
    "            VdW[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "            Vdb[l] = np.zeros((nn_structure[l],))\n",
    "    if mode==\"weights\":\n",
    "        return W, b\n",
    "    elif mode==\"gradients\":\n",
    "        return dW, db\n",
    "    elif mode==\"momentum\":\n",
    "        return VdW, Vdb\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def regularizers(W, lamda, mode=\"l2\"):\n",
    "    if mode == \"l2\":\n",
    "        l2_cost = 0.\n",
    "        for l in range(1, len(W)):\n",
    "            l2_cost += np.sum(np.square(W[l]))\n",
    "        return (lamda/2.)*l2_cost\n",
    "    return 0\n",
    "\n",
    "\n",
    "def f(z, activation=\"relu\", deri=False):\n",
    "    if deri:\n",
    "        fz_d = 0\n",
    "        if activation == \"sigmoid\":\n",
    "            fz_d = f(z, activation=\"sigmoid\") * (1 - f(z, activation=\"sigmoid\"))\n",
    "        elif activation == \"relu\":\n",
    "            fz_d = (z>=0).astype(\"int\")\n",
    "        return fz_d\n",
    "    else:\n",
    "        fz = 0\n",
    "        if activation == \"sigmoid\":\n",
    "            fz = 1 / (1 + np.exp(-z))\n",
    "        elif activation == \"relu\":\n",
    "            fz = np.maximum(0, z)\n",
    "        return fz\n",
    "\n",
    "\n",
    "def forward_prop(x, W, b, acti=\"relu\"):\n",
    "    a = {1: x}\n",
    "    z = {} \n",
    "    for l in range(1, len(W) + 1):\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  \n",
    "        a[l+1] = f(z[l+1], activation=acti) \n",
    "    return a, z\n",
    "\n",
    "\n",
    "def back_prop(y, a, z, W, b, dW, db, acti=\"relu\", regularizer=[0.01, \"l2\"]):\n",
    "    delta = {}\n",
    "    cost = 0\n",
    "    for l in range(len(a), 0, -1):\n",
    "        if l == len(a):\n",
    "            delta[l] = -(y-a[l]) * f(z[l], activation=acti, deri=True) \n",
    "            cost = np.linalg.norm((y-a[l]))\n",
    "            if regularizer and len(regularizer) == 2:\n",
    "                 cost += regularizers(W,regularizer[0],mode=regularizer[1])\n",
    "        else:\n",
    "            if l > 1:\n",
    "                delta[l] = np.dot(np.transpose(W[l]), delta[l+1]) * f(z[l], activation=acti, deri=True)\n",
    "                \n",
    "            dW[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))\n",
    "            db[l] += delta[l+1]\n",
    "    return dW, db, cost\n",
    "\n",
    "\n",
    "def plot_cost(cost):\n",
    "    plt.plot(cost)\n",
    "    plt.ylabel('Average J')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum(VdW, dW, Vdb, db, beta=0.9):\n",
    "    for l in range(1, len(dW)+1):\n",
    "        VdW[l] = beta*VdW[l] + (1.-beta)*dW[l]\n",
    "        Vdb[l] = beta*Vdb[l] + (1.-beta)*db[l]\n",
    "    return VdW, Vdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nn_structure, X, y, params, verbose=0):\n",
    "    [ext, activation, batch_size, epochs, alpha, reg] = params\n",
    "    cnt = 1\n",
    "    N = len(y)\n",
    "    if not batch_size:\n",
    "        batch_size = N\n",
    "    avg_cost_func = []\n",
    "    if verbose==1:\n",
    "        print('Starting gradient descent for {} iterations\\n\\n'.format(epochs))    \n",
    "    while cnt <= epochs:\n",
    "        if verbose==1:\n",
    "            print('Epoch {} of {}\\n'.format(cnt, epochs))\n",
    "        \n",
    "        if cnt == 1:\n",
    "             W, b = initializers(nn_structure, acti=activation, mode=\"weights\")\n",
    "        epoch_cost = 0\n",
    "        for j in range(0, N, batch_size):\n",
    "            avg_cost = 0        \n",
    "            dW, db = initializers(nn_structure, acti=activation, mode=\"gradients\")\n",
    "            if ext and ext[\"name\"] == \"momentum\":\n",
    "                VdW, Vdb = initializers(nn_structure, mode=\"momentum\")\n",
    "\n",
    "            for i in range(j, min(j+batch_size,N), 1):\n",
    "                a, z = forward_prop(X[i, :], W, b, acti=activation)\n",
    "                dW, db, cost = back_prop(y[i,:], a, z, W, b, dW, db, acti=activation, regularizer=reg)\n",
    "                avg_cost += cost\n",
    "                if ext and ext[\"name\"] == \"momentum\":\n",
    "                    VdW, Vdb = add_momentum(VdW, dW, Vdb, db, ext[\"params\"][\"beta\"])\n",
    "\n",
    "            for l in range(len(nn_structure) - 1, 0, -1):\n",
    "                if ext and ext[\"name\"] == \"momentum\":\n",
    "                    W[l] += -alpha * (1.0/batch_size * VdW[l] + reg[0]*W[l])\n",
    "                    b[l] += -alpha * (1.0/batch_size * Vdb[l])\n",
    "                else:\n",
    "                    W[l] += -alpha * (1.0/batch_size * dW[l] + reg[0]*W[l])\n",
    "                    b[l] += -alpha * (1.0/batch_size * db[l])\n",
    "\n",
    "            avg_cost = 1.0/batch_size * avg_cost\n",
    "            epoch_cost += avg_cost\n",
    "            \n",
    "        avg_cost_func.append(epoch_cost/float(N/batch_size))\n",
    "        cnt += 1\n",
    "    if verbose>=1:\n",
    "        plot_cost(avg_cost_func)\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def evaluate(X, y, W, b, n_layers, verbose=0):\n",
    "    N = X.shape[0]\n",
    "    y_pred = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = forward_prop(X[i, :], W, b)\n",
    "        y_pred[i] = np.argmax(a[n_layers])\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print('Prediction accuracy = {:0.2f} %'.format(accuracy * 100))\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "def compile_nn(source=\"sk\", ext=None, lr=0.1, num_runs=1, batch_size=None, epochs=300, verbose=2):\n",
    "    nn_structure = [64 if source==\"sk\" else 784, 30, 10]\n",
    "    params = [ext, \"relu\", batch_size, epochs, lr, [0.01,\"l2\"]]\n",
    "    [_, activation,_, epochs, alpha, reg] = params\n",
    "    \n",
    "    if verbose>=1:\n",
    "        print(\"\\n---------------------------\\n\")\n",
    "        print(\"Training Nerual Network with - \\n\\nStructure - {}\\nExtension - {}\\nActivation Function - {}\\nBatch Size - {}\\nEpochs - {}\\nLearning Rate - {}\\nRegularization - {}\".format(nn_structure, ext if ext else \"Naive\", activation, batch_size if batch_size else \"Full\", epochs, alpha, reg))\n",
    "        print(\"\\n---------------------------\\n\")\n",
    "    elif ext:\n",
    "        print(\"Training with extesnsion - {}\".format(ext))\n",
    "    \n",
    "    avg_accuracy = []\n",
    "    \n",
    "    for i in range(1, num_runs+1):\n",
    "        (X_train, y_train), (X_test, y_test) = load_mnist_data(source)\n",
    "        W, b, avg_cost_func = train(nn_structure, X_train, y_train, params, verbose)\n",
    "        avg_accuracy.append(evaluate(X_test, y_test, W, b, 3, verbose))\n",
    "    \n",
    "    if num_runs > 1:\n",
    "        print(\"Average accuracy over {} runs = {:0.2f} %\".format(num_runs, (np.sum(np.array(avg_accuracy))/float(num_runs)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy = 88.60 %\n",
      "Prediction accuracy = 88.46 %\n",
      "Prediction accuracy = 88.73 %\n",
      "Prediction accuracy = 89.29 %\n",
      "Prediction accuracy = 75.94 %\n",
      "Average accuracy over 5 runs = 86.20 %\n"
     ]
    }
   ],
   "source": [
    "compile_nn(source=\"sk\", ext=None, num_runs=5, batch_size=32, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with extesnsion - {'name': 'momentum', 'params': {'beta': 0.9}}\n",
      "Prediction accuracy = 91.66 %\n",
      "Prediction accuracy = 97.36 %\n",
      "Prediction accuracy = 77.61 %\n",
      "Prediction accuracy = 86.51 %\n",
      "Prediction accuracy = 82.34 %\n",
      "Average accuracy over 5 runs = 87.09 %\n"
     ]
    }
   ],
   "source": [
    "ext={\"name\":\"momentum\", \"params\":{\"beta\":0.9}}\n",
    "compile_nn(source=\"sk\", ext=ext, lr=0.1, num_runs=5, batch_size=32, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy = 83.97 %\n",
      "Prediction accuracy = 93.24 %\n",
      "Prediction accuracy = 80.93 %\n",
      "Prediction accuracy = 85.15 %\n",
      "Prediction accuracy = 84.20 %\n",
      "Average accuracy over 5 runs = 85.50 %\n"
     ]
    }
   ],
   "source": [
    "compile_nn(source=\"tf\", ext=None, num_runs=5, batch_size=512, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with extesnsion - {'name': 'momentum', 'params': {'beta': 0.9}}\n",
      "Prediction accuracy = 93.01 %\n",
      "Prediction accuracy = 85.07 %\n",
      "Prediction accuracy = 93.17 %\n",
      "Prediction accuracy = 84.78 %\n",
      "Prediction accuracy = 93.02 %\n",
      "Average accuracy over 5 runs = 89.81 %\n"
     ]
    }
   ],
   "source": [
    "ext={\"name\":\"momentum\", \"params\":{\"beta\":0.9}}\n",
    "compile_nn(source=\"tf\", ext=ext, lr=0.1, num_runs=5, batch_size=512, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
